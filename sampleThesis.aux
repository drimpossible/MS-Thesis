\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\newlabel{ch:ack}{{}{v}{Acknowledgments}{chapter*.1}{}}
\newlabel{ch:abstract}{{}{vi}{Abstract}{chapter*.2}{}}
\citation{horowitz2014power}
\citation{bojarski2016end}
\citation{you2016image,johnson2016densecap}
\citation{mishra2012top,neumann2012real}
\citation{taigman2014deepface}
\citation{alex2012alexnet}
\citation{simonyan2014very}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces There are two significant roadblocks to efficient deployment of large CNN models: realtime computation, and in-memory compactness. Cloud is not an option due to these real-time operation constraints.\relax }}{1}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:challenges}{{1.1}{1}{There are two significant roadblocks to efficient deployment of large CNN models: realtime computation, and in-memory compactness. Cloud is not an option due to these real-time operation constraints.\relax }{figure.caption.6}{}}
\citation{he2016deep,iandola2016squeezenet}
\citation{moczulski2015acdc,yang2015deep}
\citation{han2015deep,liu2017learning}
\citation{courbariaux2016binarized,zhou2016dorefa}
\citation{yang2015deep}
\citation{cheng2015exploration}
\citation{yang2015deep}
\citation{collins2014memory}
\citation{zhou2016less}
\citation{moczulski2015acdc}
\citation{han2015deep}
\citation{han2015deep}
\citation{yu2017compressing}
\citation{srinivas2017training}
\citation{guo2016dynamic}
\citation{han2015deep}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Comparison of Binarization and other methods in terms of compression.\relax }}{2}{table.caption.7}}
\newlabel{table:versions_typesofcompression}{{1.1}{2}{Comparison of Binarization and other methods in terms of compression.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Quantization}{2}{section.1.1}}
\citation{rastegari2016xnor,courbariaux2016binarized}
\citation{rastegari2016xnor,courbariaux2016binarized}
\citation{rastegari2016xnor}
\citation{horowitz2014power}
\citation{horowitz2014power}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces As shown by Horowitz \emph  {et al}\onedot  \cite  {horowitz2014power}, power consumption for various operations at 45nm 0.9V. Observe that 8-bit integers require significantly less energy than their equivalent 32-bit floating point operations.\relax }}{3}{table.caption.8}}
\newlabel{table:mac-energy}{{1.2}{3}{As shown by Horowitz \etal \cite {horowitz2014power}, power consumption for various operations at 45nm 0.9V. Observe that 8-bit integers require significantly less energy than their equivalent 32-bit floating point operations.\relax }{table.caption.8}{}}
\citation{he2016deep}
\citation{huang2017densely}
\citation{howard2017mobilenets}
\citation{spielman2007spectral}
\citation{salil2012pseudo}
\citation{hoory2006expander}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Compact layers (Pruning)}{4}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions}{5}{section.1.3}}
\citation{courbariaux2016binarized,zhou2016dorefa,han2015deep,liu2017learning,moczulski2015acdc,yang2015deep,he2016deep,iandola2016squeezenet}
\citation{cheng2018recent,sze2017efficient}
\citation{lecun1990optimal}
\citation{hassibi1993optimal}
\citation{liu2017learning}
\citation{sze2017efficient}
\citation{han2015learning}
\citation{wen2016structured}
\citation{guo2016dynamic}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{7}{chapter.2}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:related}{{2}{7}{Related Work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Network Pruning}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Pixel and Vector Pruning}{7}{subsection.2.1.1}}
\citation{srinivas2015data}
\citation{li2016pruning}
\citation{luo2017thinet}
\citation{he2017channel}
\citation{liu2017learning}
\citation{lebedev2016fast}
\citation{scardapane2017group}
\citation{guo2018survey}
\citation{chen2015compressing}
\citation{han2015deep}
\citation{han2015learning}
\citation{zhou2017inq}
\citation{zhu2016trained}
\citation{li2016ternary}
\citation{courbariaux2015binaryconnect}
\citation{courbariaux2016binarized}
\citation{hubara2017quantized}
\citation{rastegari2016xnor}
\citation{hubara2017quantized}
\citation{zhou2016dorefa}
\citation{cai2017deep}
\citation{cai2017deep}
\citation{lin2016fixed}
\citation{tang2017train}
\citation{hou2016loss}
\citation{anderson2017high}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Channel and Filter Pruning}{8}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Quantization}{8}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Higher bit quantization}{8}{subsection.2.2.1}}
\citation{merolla2016deep}
\citation{juefei2016local}
\citation{bagherinezhad2016lcnn}
\citation{sainath2013low}
\citation{novikov2015tensorizing,masana2017domain}
\citation{jaderberg2014speeding}
\citation{denil2013predicting}
\citation{kim2015compression}
\citation{lebedev2014speeding}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}1-bit quantization}{9}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Other network compression approaches}{9}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Low-rank Approximations}{9}{subsection.2.3.1}}
\citation{yu2015sketch}
\citation{yu2015sketch}
\citation{yu2015sketch,yu2017sketch}
\citation{bui2016sbir}
\citation{yu2016shoe}
\citation{wang2016crossir}
\citation{iandola2016squeezenet,howard2017mobilenets,hu2017squeeze}
\citation{szegedy2015going}
\citation{liu2017hierarchical,zoph2017learning,zhong2017practical,liu2017progressive}
\citation{alex2012alexnet}
\citation{howard2017mobilenets}
\citation{chollet2017xception}
\citation{zhang2018shufflenet,sandler2018inverted,huang2018condensenet}
\citation{he2016deep,huang2017densely}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Sketch Recognition}{10}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Efficient Architecture Design}{10}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Novelty}{10}{section.2.4}}
\citation{zhou2017inq,li2016ternary,li2016ternary}
\citation{lin2017does,rolnick2017power}
\citation{cybenko1989approximation}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Distribution Aware Binarization of Networks}{12}{chapter.3}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:dabn}{{3}{12}{Distribution Aware Binarization of Networks}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example sketch passing through a convolutional layer filter, with the real-valued filter shown alongside corresponding $\alpha $-$\beta $ and XNOR-Net filters. Orange signifies the highest response areas. We can see that DAB-Net has significantly better responses when compared to XNOR-Net\relax }}{12}{figure.caption.9}}
\newlabel{fig:alphabetadiagram}{{3.1}{12}{An example sketch passing through a convolutional layer filter, with the real-valued filter shown alongside corresponding $\alpha $-$\beta $ and XNOR-Net filters. Orange signifies the highest response areas. We can see that DAB-Net has significantly better responses when compared to XNOR-Net\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Representational Power of Binary Networks}{12}{section.3.1}}
\citation{lin2017does}
\citation{rolnick2017power}
\newlabel{theorem:binaryrepresentation}{{1}{13}{Representational Power of Binary Networks}{theorem.3.1.1}{}}
\newlabel{eqn:constantlayers}{{3.1}{13}{Representational Power of Binary Networks}{equation.3.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Distribution-Aware Binarization}{14}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Derivation}{14}{subsection.3.2.1}}
\newlabel{approx}{{3.2.1}{14}{Derivation}{subsection.3.2.1}{}}
\citation{rastegari2016xnor}
\citation{courbariaux2016binarized}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Intuitions about DAB-Net}{15}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Implementation}{15}{subsection.3.2.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Finding an optimal K value. \relax }}{16}{algorithm.1}}
\newlabel{alg:partitionalgo}{{1}{16}{Finding an optimal K value. \relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Parallel Prefix-Sums to Obtain $K$}{16}{section*.10}}
\newlabel{theorem:approx}{{3}{16}{Parallel Prefix-Sums to Obtain $K$}{theorem.3.2.3}{}}
\citation{rastegari2016xnor}
\@writefile{toc}{\contentsline {subsubsection}{Forward and Backward Pass}{17}{section*.11}}
\newlabel{corollary:forward}{{1}{17}{Weight Binarization}{corollary.1}{}}
\newlabel{theorem:backward}{{4}{17}{Backward Pass}{theorem.3.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Training Procedure}{17}{subsection.3.2.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Training an $L$-layers CNN with binary weights:\relax }}{18}{algorithm.2}}
\newlabel{alg:trainbinconv}{{2}{18}{Training an $L$-layers CNN with binary weights:\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experiments}{18}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Experimental Setup}{18}{subsection.3.3.1}}
\citation{eitz2012hdhso}
\citation{yu2015sketch}
\citation{he2016deep}
\citation{szegedy2015going}
\citation{yu2017sketch}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Results}{19}{subsection.3.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Our DAB-Net models compared to FBin, WBin and FPrec models on TU-Berlin and Sketchy in terms of accuracy.\relax }}{20}{table.caption.12}}
\newlabel{table:tub_recacc}{{3.1}{20}{Our DAB-Net models compared to FBin, WBin and FPrec models on TU-Berlin and Sketchy in terms of accuracy.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}XNOR-Net vs DAB-Net}{20}{subsection.3.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces A comparison between state-of-the-art single model accuracies of recognition systems on the TU-Berlin dataset.\relax }}{21}{table.caption.13}}
\newlabel{table:sketchcomp}{{3.2}{21}{A comparison between state-of-the-art single model accuracies of recognition systems on the TU-Berlin dataset.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variation of $\alpha $ and $\beta $ across Time}{21}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Variation of $K$ across Time and Layers}{21}{section*.17}}
\newlabel{sec:kacrosslayers}{{3.3.3}{21}{Variation of $K$ across Time and Layers}{section*.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sub-figures (1) to (4) show the train-time variation of $\alpha $ and $\beta $ for a layer filter. Initially, $\alpha $ and $\beta $ have nearly equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that $\alpha $ and $\beta $ have widely different magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.\relax }}{22}{figure.caption.14}}
\newlabel{fig:alphabetaovertime}{{3.2}{22}{Sub-figures (1) to (4) show the train-time variation of $\alpha $ and $\beta $ for a layer filter. Initially, $\alpha $ and $\beta $ have nearly equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that $\alpha $ and $\beta $ have widely different magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Summary}{22}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces (1) shows the variation of the normalized K-value over time during training. It falls initially but converges eventually to 0.35. The normalized K-value for XNOR-Net remains almost at 0.5 till the end. (2) shows the variation of normalized K values on random filters across layers. The K-value corresponding to DAB-Net varies across layers based on the distribution of weights of the specific layer, which is not captured by XNOR-Net.\relax }}{23}{figure.caption.15}}
\newlabel{fig:alphabetaovertime}{{3.3}{23}{(1) shows the variation of the normalized K-value over time during training. It falls initially but converges eventually to 0.35. The normalized K-value for XNOR-Net remains almost at 0.5 till the end. (2) shows the variation of normalized K values on random filters across layers. The K-value corresponding to DAB-Net varies across layers based on the distribution of weights of the specific layer, which is not captured by XNOR-Net.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Hybrid Binary Networks}{24}{chapter.4}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:hbn}{{4}{24}{Hybrid Binary Networks}{chapter.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Convolution of binary and non-binary activations of two different layers. Note that the error introduced due to binarization is minimal in the first pair compared to the second. Hence, efficiently deciding \textit  {which} layers to binarize could contribute significantly to the overall accuracy of the network and not damage the speed-ups.\relax }}{24}{figure.caption.18}}
\newlabel{fig:introdiag}{{4.1}{24}{Convolution of binary and non-binary activations of two different layers. Note that the error introduced due to binarization is minimal in the first pair compared to the second. Hence, efficiently deciding \textit {which} layers to binarize could contribute significantly to the overall accuracy of the network and not damage the speed-ups.\relax }{figure.caption.18}{}}
\citation{rastegari2016xnor}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Binarization-error metric across layers for Sketch-A-Net, ResNet-18, and SqueezeNet. Stars indicate that the layer was replaced with a WeightBinConv layer, while squares indicate the FullBinConv layer was retained in the FBin model. We see that the algorithm selects the last layers in the case of Sketch-A-Net and ResNet, while in the case of SqueezeNet, it selects the first four, last three and some alternate intermediate layers to be replaced by WeightBinConv layers, retaining the rest as FullBinConv layers.\relax }}{25}{figure.caption.19}}
\newlabel{fig:metric}{{4.2}{25}{Binarization-error metric across layers for Sketch-A-Net, ResNet-18, and SqueezeNet. Stars indicate that the layer was replaced with a WeightBinConv layer, while squares indicate the FullBinConv layer was retained in the FBin model. We see that the algorithm selects the last layers in the case of Sketch-A-Net and ResNet, while in the case of SqueezeNet, it selects the first four, last three and some alternate intermediate layers to be replaced by WeightBinConv layers, retaining the rest as FullBinConv layers.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hybrid Binarization}{25}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Error Metric: Optimizing Speed \& Accuracy}{25}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Partitioning Algorithm}{26}{subsection.4.1.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces {\bf  Partition Algorithm}  Marks layers for binarization and creates a hybrid network.\relax }}{27}{algorithm.3}}
\newlabel{alg:partitionalgo}{{3}{27}{{\bf Partition Algorithm} \\Marks layers for binarization and creates a hybrid network.\relax }{algorithm.3}{}}
\citation{courbariaux2016binarized}
\citation{alex2012alexnet}
\citation{he2016deep}
\citation{eitz2012hdhso}
\citation{iandola2016squeezenet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The Procedure: Error metrics from binarization of inputs to the network layers are partitioned into clusters using K-means. The highest error cluster indicates the inputs that are not binarized to generate the hybrid version.\relax }}{28}{figure.caption.20}}
\newlabel{fig:pipeline}{{4.3}{28}{The Procedure: Error metrics from binarization of inputs to the network layers are partitioned into clusters using K-means. The highest error cluster indicates the inputs that are not binarized to generate the hybrid version.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Impact on Speed and Energy Use}{28}{subsection.4.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Experiments and Results}{28}{section.4.2}}
\citation{deng2009imagenet}
\citation{yu2015sketch}
\citation{eitz2012hdhso}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Datasets and Models}{29}{subsection.4.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces  A detailed comparison of accuracy, memory use, FLOPs with popular benchmark compression techniques on ImageNet. Our hybrid models outperform other 1-bit activation models and perform on par with 2-bit models while having a significantly higher speedup. Hybrid-2 models have the last layer binarized.\relax }}{30}{table.caption.21}}
\newlabel{table:imagenet_fullcomp}{{4.1}{30}{A detailed comparison of accuracy, memory use, FLOPs with popular benchmark compression techniques on ImageNet. Our hybrid models outperform other 1-bit activation models and perform on par with 2-bit models while having a significantly higher speedup. Hybrid-2 models have the last layer binarized.\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Results}{30}{subsection.4.2.2}}
\citation{yu2017sketch}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Our hybrid models compared to FBin, WBin and NoBin models on Imagenet in terms of accuracy, memory and computations expense.\relax }}{31}{table.caption.22}}
\newlabel{table:versions_imagenet}{{4.2}{31}{Our hybrid models compared to FBin, WBin and NoBin models on Imagenet in terms of accuracy, memory and computations expense.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Algorithmic Insights}{31}{subsection.4.2.3}}
\citation{courbariaux2016binarized}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Our hybrid models compared to FBin, WBin and full prec models on TU-Berlin and Sketchy datasets in terms of accuracy, memory and speed tradeoff.\relax }}{32}{table.caption.23}}
\newlabel{table:tub_recacc}{{4.3}{32}{Our hybrid models compared to FBin, WBin and full prec models on TU-Berlin and Sketchy datasets in terms of accuracy, memory and speed tradeoff.\relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces A comparison between state-of-the-art single model accuracies of recognition systems on the TU-Berlin dataset.\relax }}{32}{table.caption.25}}
\newlabel{table:sketchcomp}{{4.4}{32}{A comparison between state-of-the-art single model accuracies of recognition systems on the TU-Berlin dataset.\relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Why are layer-wise errors independent?}{32}{subsection.4.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Trade-off between WeightBinConv layers and accuracy on the TU-Berlin dataset is shown in the left figure, while the trade-off between weight binarized layers and speedup is shown in the right figure. Early on, we observe that a small increase in the percentage of WeightBinConv layers leads to a large increase in accuracy and a marginal decrease in speed. We achieve accuracies comparable to the WBin model with much fewer WeightBinConv layers. \relax }}{33}{figure.caption.24}}
\newlabel{fig:tradeoff}{{4.4}{33}{Trade-off between WeightBinConv layers and accuracy on the TU-Berlin dataset is shown in the left figure, while the trade-off between weight binarized layers and speedup is shown in the right figure. Early on, we observe that a small increase in the percentage of WeightBinConv layers leads to a large increase in accuracy and a marginal decrease in speed. We achieve accuracies comparable to the WBin model with much fewer WeightBinConv layers. \relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Variation with the Hybridization Ratio ($R$)}{33}{subsection.4.2.5}}
\citation{zhou2016dorefa,tang2017train}
\citation{tang2017train}
\citation{anderson2017high}
\citation{iandola2016squeezenet}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Effects of last layer weight-binarization on TU-Berlin dataset, for Sketch-A-Net and ResNet-1. Observe that our hybrid models do not face drastic accuracy drop when the last layer is weight-binarized.\relax }}{34}{table.caption.26}}
\newlabel{table:otherresults}{{4.5}{34}{Effects of last layer weight-binarization on TU-Berlin dataset, for Sketch-A-Net and ResNet-1. Observe that our hybrid models do not face drastic accuracy drop when the last layer is weight-binarized.\relax }{table.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Optimizing Memory}{34}{subsection.4.2.6}}
\citation{iandola2016squeezenet}
\citation{cai2017deep}
\citation{tang2017train}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Our performance on SqueezeNet, an explicitly compressed model architecture. Although SqueezeNet is an inherently compressed model, our method still achieves further compression on it.\relax }}{35}{table.caption.27}}
\newlabel{table:squeezenet}{{4.6}{35}{Our performance on SqueezeNet, an explicitly compressed model architecture. Although SqueezeNet is an inherently compressed model, our method still achieves further compression on it.\relax }{table.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.7}Compressing Compact Models}{35}{subsection.4.2.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Summary}{35}{section.4.3}}
\citation{he2016deep}
\citation{huang2017densely}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Deep Expander Networks}{37}{chapter.5}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:den}{{5}{37}{Deep Expander Networks}{chapter.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Popular sparse approximations are agnostic to the global information flow in a network, possibly creating disconnected components. In contrast, expander graph-based models produce sparse yet highly connected networks.\relax }}{37}{figure.caption.28}}
\newlabel{fig:intro}{{5.1}{37}{Popular sparse approximations are agnostic to the global information flow in a network, possibly creating disconnected components. In contrast, expander graph-based models produce sparse yet highly connected networks.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Approach}{37}{section.5.1}}
\newlabel{sec:approach}{{5.1}{37}{Approach}{section.5.1}{}}
\citation{salil2012pseudo}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Graphs and Deep CNNs}{38}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Sparse Random Graphs}{38}{subsection.5.1.2}}
\citation{salil2012pseudo}
\citation{salil2012pseudo}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Measures of Connectivity}{39}{subsection.5.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The proposed fast convolution algorithm for X-Conv layer. We represent all the non-zero filters in the weight matrix of the X-Conv layer as a compressed dense matrix of $D$ channels. The algorithm starts by selecting $D$ channels from input (with replacement) using a mask created while initializing the model. The output is computed by convolving these selected channels with the compressed weight matrices.\relax }}{40}{figure.caption.29}}
\newlabel{fig:efficientmatrix}{{5.2}{40}{The proposed fast convolution algorithm for X-Conv layer. We represent all the non-zero filters in the weight matrix of the X-Conv layer as a compressed dense matrix of $D$ channels. The algorithm starts by selecting $D$ channels from input (with replacement) using a mask created while initializing the model. The output is computed by convolving these selected channels with the compressed weight matrices.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Sensitivity of X-Nets}{40}{subsection.5.1.4}}
\newlabel{thm:conn}{{5}{40}{Sensitivity of X-Nets}{theorem.5.1.5}{}}
\citation{cudaconvnet}
\citation{blocksparse}
\newlabel{alg:cnnalgo}{{\caption@xref {alg:cnnalgo}{ on input line 96}}{41}{Using Sparse Representation}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Efficient Algorithms}{41}{section.5.2}}
\newlabel{sec:implementation}{{5.2}{41}{Efficient Algorithms}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Using Sparse Representation}{41}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}X-Net based Fast Dense Convolution}{41}{subsection.5.2.2}}
\citation{zhang2018shufflenet}
\citation{howard2017mobilenets}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison between Grouped convolutions and X-Conv using MobileNet architecture trained on ImageNet. X-$d$ or G-$d$ represents the 1x1 conv layers are compressed by $d$ times using X-Conv or Groups. We observe X-MobileNets beat Group-MobileNet by 4\% in accuracy on increasing sparsity.\relax }}{42}{figure.caption.30}}
\newlabel{fig:mobilenet}{{5.3}{42}{Comparison between Grouped convolutions and X-Conv using MobileNet architecture trained on ImageNet. X-$d$ or G-$d$ represents the 1x1 conv layers are compressed by $d$ times using X-Conv or Groups. We observe X-MobileNets beat Group-MobileNet by 4\% in accuracy on increasing sparsity.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experiments and Results}{42}{section.5.3}}
\newlabel{sec:experiments}{{5.3}{42}{Experiments and Results}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Comparison with Grouped Convolution}{42}{subsection.5.3.1}}
\newlabel{sec:group}{{5.3.1}{42}{Comparison with Grouped Convolution}{subsection.5.3.1}{}}
\citation{huang2017densely}
\citation{he2016deep}
\citation{deng2009imagenet}
\citation{krizhevsky2009learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces We show the error as a function of \#FLOPs during test-time (below) for DenseNet-BC with X-DenseNet-BCs on CIFAR10 and CIFAR100 datasets. We observe X-DenseNet-BCs achieve better performance tradeoffs over DenseNet-BC models. For each datapoint, we mention the X-C-D-G notation (see Section \ref  {sec:denres}) along with the accuracy. \relax }}{43}{figure.caption.31}}
\newlabel{fig:cifar}{{5.4}{43}{We show the error as a function of \#FLOPs during test-time (below) for DenseNet-BC with X-DenseNet-BCs on CIFAR10 and CIFAR100 datasets. We observe X-DenseNet-BCs achieve better performance tradeoffs over DenseNet-BC models. For each datapoint, we mention the X-C-D-G notation (see Section \ref {sec:denres}) along with the accuracy. \relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Comparison with Efficient CNN Architectures}{43}{subsection.5.3.2}}
\newlabel{sec:denres}{{5.3.2}{43}{Comparison with Efficient CNN Architectures}{subsection.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces We show the error as a function of \#FLOPs to compare between ResNet and X-ResNet on the ImageNet dataset. We observe X-ResNets achieve better performance tradeoffs over original ResNet models.\relax }}{44}{figure.caption.32}}
\newlabel{fig:resnet}{{5.5}{44}{We show the error as a function of \#FLOPs to compare between ResNet and X-ResNet on the ImageNet dataset. We observe X-ResNets achieve better performance tradeoffs over original ResNet models.\relax }{figure.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Results obtained by ResNet and DenseNet-BC models on ImageNet dataset, ordered by \#FLOPs. or each datapoint, we use the X-C-D-G notation (see Section \ref  {sec:denres}) along with the accuracy.\relax }}{44}{table.caption.33}}
\newlabel{tab:imagenet}{{5.1}{44}{Results obtained by ResNet and DenseNet-BC models on ImageNet dataset, ordered by \#FLOPs. or each datapoint, we use the X-C-D-G notation (see Section \ref {sec:denres}) along with the accuracy.\relax }{table.caption.33}{}}
\citation{li2016pruning}
\citation{liu2017learning}
\citation{collins2014memory}
\citation{zhou2016less}
\citation{han2015deep}
\citation{han2015deep}
\citation{srinivas2017training}
\citation{guo2016dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Comparison with Pruning Techniques}{45}{subsection.5.3.3}}
\newlabel{sec:prun}{{5.3.3}{45}{Comparison with Pruning Techniques}{subsection.5.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Stability of Models}{45}{subsection.5.3.4}}
\citation{salil2012pseudo}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Comparison with other methods on CIFAR-10 dataset using VGG16 as the base model. We significantly outperform popular compression techniques, achieving similar accuracies with upto 13x compression rate.\relax }}{46}{table.caption.34}}
\newlabel{tab:cifar_fullcomp}{{5.2}{46}{Comparison with other methods on CIFAR-10 dataset using VGG16 as the base model. We significantly outperform popular compression techniques, achieving similar accuracies with upto 13x compression rate.\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison with other methods on ImageNet-2012 using AlexNet as the base model. We are able to achieve comparable accuracies using only 9.7M parameters.\relax }}{46}{table.caption.35}}
\newlabel{tab:imagenet_fullcomp}{{5.3}{46}{Comparison with other methods on ImageNet-2012 using AlexNet as the base model. We are able to achieve comparable accuracies using only 9.7M parameters.\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces The accuracies (mean $\pm $ stddev) of various models over 10 training runs on CIFAR-10 dataset.\relax }}{47}{table.caption.36}}
\newlabel{tab:cifar}{{5.4}{47}{The accuracies (mean $\pm $ stddev) of various models over 10 training runs on CIFAR-10 dataset.\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces The mean accuracy and range of variation over 2 runs of MobileNet0.5 variants on ImageNet dataset.\relax }}{47}{table.caption.36}}
\newlabel{tab:imgnet}{{5.5}{47}{The mean accuracy and range of variation over 2 runs of MobileNet0.5 variants on ImageNet dataset.\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.5}Training Wider and Deeper networks}{47}{subsection.5.3.5}}
\newlabel{sec:ultrawide}{{5.3.5}{47}{Training Wider and Deeper networks}{subsection.5.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces We show the performance tradeoff obtained on training significantly wider and deeper networks on CIFAR-100 dataset. Every datapoint is X-$C$ specified along with the number of parameters, $C$ being the compression factor. We show that training wider or deeper networks along with more compression using X-Nets achieve better accuracies with upto two-thirds of the total parameter and FLOPs on CIFAR-100 dataset. \relax }}{48}{figure.caption.37}}
\newlabel{fig:deepnet}{{5.6}{48}{We show the performance tradeoff obtained on training significantly wider and deeper networks on CIFAR-100 dataset. Every datapoint is X-$C$ specified along with the number of parameters, $C$ being the compression factor. We show that training wider or deeper networks along with more compression using X-Nets achieve better accuracies with upto two-thirds of the total parameter and FLOPs on CIFAR-100 dataset. \relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Summary}{48}{section.5.4}}
\citation{bojarski2016end}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{49}{chapter.6}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:wrapup}{{6}{49}{Conclusions}{chapter.6}{}}
\newlabel{ch:relatedPubs}{{6}{51}{Related Publications}{chapter*.38}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {}{\it  Appendix}\ {\it  A}:\ Binary Networks: Appendix}{52}{appendix.A}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:apndx1}{{A}{52}{Binary Networks: Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Optimal representation of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle  \mathbf  {W}$}\mathaccent "0365{\mathbf  {W}}$}{52}{section.A.1}}
\newlabel{approx}{{1}{52}{Optimal representation of $\widetilde {\mathbf {W}}$}{name2.1}{}}
\newlabel{wformulation}{{A.3}{52}{Optimal representation of $\widetilde {\mathbf {W}}$}{equation.A.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Gradient derivation}{53}{section.A.2}}
\citation{lin2017does}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Binary Networks as Approximators}{55}{section.A.3}}
\newlabel{eqn:constantlayers}{{A.13}{55}{Binary Networks as Approximators}{equation.A.3.13}{}}
\newlabel{thm:constantlayers}{{7}{55}{Binary Networks as Approximators}{equation.A.3.13}{}}
\newlabel{eqn:branching}{{A.14}{55}{Binary Networks as Approximators}{equation.A.3.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Expressibility of Binary Networks}{55}{section.A.4}}
\citation{lin2017does}
\citation{lin2017does}
\newlabel{goal}{{A.15}{56}{Expressibility of Binary Networks}{equation.A.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Experimental details}{56}{section.A.5}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Experimental details}{56}{section.A.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6.1}Data processing}{56}{subsection.A.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6.2}Hyper-parameters}{57}{subsection.A.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}FLOPs, Exploiting Filter Repetition and Computational Cost Calculation}{57}{section.A.7}}
\@writefile{toc}{\contentsline {section}{\numberline {A.8}Models used}{58}{section.A.8}}
\@writefile{toc}{\contentsline {section}{\numberline {A.9}Model Architectures}{58}{section.A.9}}
\@writefile{toc}{\contentsline {section}{\numberline {A.10}Binarization-errors across layers }{58}{section.A.10}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Layers of the AlexNet model, with the number of parameters and FLOPs for versions (WBin, Fbin, Hybrid, FPrec) of each. Also, the amount of unique parameters (a high number indicating high compressibility) is shown for each layer.\relax }}{59}{table.caption.39}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Layer descriptions of the Sketch-A-Net model.\relax }}{59}{table.caption.40}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Layers descriptions of the ResNet-18 model.\relax }}{60}{table.caption.41}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Layers descriptions of the SqueezeNet model.\relax }}{61}{table.caption.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of Sketch-A-Net. Our hybrid versions replace most conv layers with FullBinConv layers, but replace layers towards the end with WeightBinConv layers, following the algorithm.\relax }}{63}{figure.caption.43}}
\newlabel{fig:sketchanet}{{A.1}{63}{Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of Sketch-A-Net. Our hybrid versions replace most conv layers with FullBinConv layers, but replace layers towards the end with WeightBinConv layers, following the algorithm.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of ResNet-18.\relax }}{64}{figure.caption.44}}
\newlabel{fig:resnet}{{A.2}{64}{Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of ResNet-18.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of SqueezeNet.\relax }}{65}{figure.caption.45}}
\newlabel{fig:squeezenet}{{A.3}{65}{Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of SqueezeNet.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of AlexNet.\relax }}{66}{figure.caption.46}}
\newlabel{fig:alexnet}{{A.4}{66}{Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of AlexNet.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {}{\it  Appendix}\ {\it  B}:\ Deep Expander Networks: Appendix}{67}{appendix.B}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:apndx2}{{B}{67}{Deep Expander Networks: Appendix}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Explicit Expanders}{67}{section.B.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Cayley Expander}{67}{subsection.B.1.1}}
\citation{salil2012pseudo}
\citation{salil2012pseudo}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Sensitivity in Expanders}{68}{section.B.2}}
\newlabel{lab:spec}{{9}{68}{Theorem 4.6 \cite {salil2012pseudo}}{theorem.B.2.9}{}}
\newlabel{lab:exp-mix-lem}{{10}{68}{Lemma 4.15 \cite {salil2012pseudo}}{theorem.B.2.10}{}}
\newlabel{thm:conn}{{11}{68}{Sensitivity of X-Nets}{theorem.B.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Model Details}{69}{section.B.3}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Filter sizes for the AlexNet model. Notice the filter sizes of the linear layers of the original model has $|V|\times |U|$ parameters, whereas X-AlexNet models have $|V|\times D$ parameters. Note that $D << |U|$ as stated in Section 3.2. Hence, expander graphs model connections in linear layers (X-Linear) effectively.\relax }}{69}{table.caption.47}}
\newlabel{tab:vgg}{{B.1}{69}{Filter sizes for the AlexNet model. Notice the filter sizes of the linear layers of the original model has $|V|\times |U|$ parameters, whereas X-AlexNet models have $|V|\times D$ parameters. Note that $D << |U|$ as stated in Section 3.2. Hence, expander graphs model connections in linear layers (X-Linear) effectively.\relax }{table.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Filter structure of AlexNet and VGG}{69}{subsection.B.3.1}}
\citation{howard2017mobilenets}
\citation{zhang2018shufflenet}
\citation{howard2017mobilenets}
\citation{zhang2017shufflenet}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Filter sizes for the VGG-16 model on CIFAR-10 dataset. The filter sizes given are $|V|\times |U| \times c \times c$ in original VGG network, $|V|\times D \times c \times c$ in our X-VGG16 models. Note that $D << |U|$ as stated in Section 3.2. Hence, expander graphs model connections in Convolutional layers (X-Conv) effectively.\relax }}{70}{table.caption.48}}
\newlabel{tab:alexnet}{{B.2}{70}{Filter sizes for the VGG-16 model on CIFAR-10 dataset. The filter sizes given are $|V|\times |U| \times c \times c$ in original VGG network, $|V|\times D \times c \times c$ in our X-VGG16 models. Note that $D << |U|$ as stated in Section 3.2. Hence, expander graphs model connections in Convolutional layers (X-Conv) effectively.\relax }{table.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Results}{70}{subsection.B.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Results obtained on the state-of-the-art models on CIFAR-10 and CIFAR-100 datasets, ordered by FLOPs per model. X-Nets give significantly better accuracies with corresponding DenseNet models in the same limited computational budget and correspondingly significant parameter and FLOP reduction for models with similar accuracy.\relax }}{71}{table.caption.49}}
\newlabel{tab:cifar}{{B.3}{71}{Results obtained on the state-of-the-art models on CIFAR-10 and CIFAR-100 datasets, ordered by FLOPs per model. X-Nets give significantly better accuracies with corresponding DenseNet models in the same limited computational budget and correspondingly significant parameter and FLOP reduction for models with similar accuracy.\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces Results obtained on the state-of-the-art models on ImageNet dataset, ordered by FLOPs. We also observe that X-DenseNetBC models outperform ResNet and X-ResNet models in both compression, parameters and FLOPs and achieve comparable accuracies with the highly efficient MobileNets and ShuffleNets in the same parameter budget, albeit with much higher FLOPs due to architectural constraints.\relax }}{72}{table.caption.50}}
\newlabel{tab:imagenet}{{B.4}{72}{Results obtained on the state-of-the-art models on ImageNet dataset, ordered by FLOPs. We also observe that X-DenseNetBC models outperform ResNet and X-ResNet models in both compression, parameters and FLOPs and achieve comparable accuracies with the highly efficient MobileNets and ShuffleNets in the same parameter budget, albeit with much higher FLOPs due to architectural constraints.\relax }{table.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.3}Experimental Details}{72}{subsection.B.3.3}}
\newlabel{sec:expdetails}{{B.3.3}{72}{Experimental Details}{subsection.B.3.3}{}}
\bibstyle{latex8}
\bibdata{./sampleBib}
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces We display accuracies, parameters and FLOPs of all the wider and deeper networks on CIFAR-100 listed in increasing compression order. This proves that efficiently designing layers like X-Conv and X-Linear allows us to train wider and deeper networks frugally.\relax }}{74}{table.caption.51}}
\newlabel{tab:ultranets}{{B.5}{74}{We display accuracies, parameters and FLOPs of all the wider and deeper networks on CIFAR-100 listed in increasing compression order. This proves that efficiently designing layers like X-Conv and X-Linear allows us to train wider and deeper networks frugally.\relax }{table.caption.51}{}}
\bibcite{anderson2017high}{1}
\bibcite{zhou2017inq}{2}
\bibcite{bagherinezhad2016lcnn}{3}
\bibcite{bojarski2016end}{4}
\bibcite{bui2016sbir}{5}
\bibcite{cai2017deep}{6}
\bibcite{chen2015compressing}{7}
\bibcite{cheng2018recent}{8}
\bibcite{cheng2015exploration}{9}
\bibcite{zhu2016trained}{10}
\bibcite{chollet2017xception}{11}
\bibcite{collins2014memory}{12}
\bibcite{courbariaux2015binaryconnect}{13}
\bibcite{courbariaux2016binarized}{14}
\bibcite{cybenko1989approximation}{15}
\bibcite{deng2009imagenet}{16}
\@writefile{toc}{{\@tempskipb 3.0ex plus 1pt\relax }}
\@writefile{toc}{\contentsline {chapter}{{Bibliography}}{75}{appendix*.52}}
\bibcite{denil2013predicting}{17}
\bibcite{eitz2012hdhso}{18}
\bibcite{blocksparse}{19}
\bibcite{guo2018survey}{20}
\bibcite{guo2016dynamic}{21}
\bibcite{han2015deep}{22}
\bibcite{han2015learning}{23}
\bibcite{hassibi1993optimal}{24}
\bibcite{he2016deep}{25}
\bibcite{he2017channel}{26}
\bibcite{hoory2006expander}{27}
\bibcite{horowitz2014power}{28}
\bibcite{hou2016loss}{29}
\bibcite{howard2017mobilenets}{30}
\bibcite{hu2017squeeze}{31}
\bibcite{huang2018condensenet}{32}
\bibcite{huang2017densely}{33}
\bibcite{hubara2017quantized}{34}
\bibcite{iandola2016squeezenet}{35}
\bibcite{jaderberg2014speeding}{36}
\bibcite{johnson2016densecap}{37}
\bibcite{juefei2016local}{38}
\bibcite{kim2015compression}{39}
\bibcite{cudaconvnet}{40}
\bibcite{krizhevsky2009learning}{41}
\bibcite{alex2012alexnet}{42}
\bibcite{lebedev2014speeding}{43}
\bibcite{lebedev2016fast}{44}
\bibcite{lecun1990optimal}{45}
\bibcite{li2016ternary}{46}
\bibcite{li2016pruning}{47}
\bibcite{lin2016fixed}{48}
\bibcite{lin2017does}{49}
\bibcite{liu2017progressive}{50}
\bibcite{liu2017hierarchical}{51}
\bibcite{liu2017learning}{52}
\bibcite{luo2017thinet}{53}
\bibcite{masana2017domain}{54}
\bibcite{merolla2016deep}{55}
\bibcite{mishra2012top}{56}
\bibcite{moczulski2015acdc}{57}
\bibcite{neumann2012real}{58}
\bibcite{novikov2015tensorizing}{59}
\bibcite{rastegari2016xnor}{60}
\bibcite{rolnick2017power}{61}
\bibcite{sainath2013low}{62}
\bibcite{sandler2018inverted}{63}
\bibcite{scardapane2017group}{64}
\bibcite{simonyan2014very}{65}
\bibcite{spielman2007spectral}{66}
\bibcite{srinivas2015data}{67}
\bibcite{srinivas2017training}{68}
\bibcite{sze2017efficient}{69}
\bibcite{szegedy2015going}{70}
\bibcite{taigman2014deepface}{71}
\bibcite{tang2017train}{72}
\bibcite{salil2012pseudo}{73}
\bibcite{wang2016crossir}{74}
\bibcite{wen2016structured}{75}
\bibcite{yang2015deep}{76}
\bibcite{you2016image}{77}
\bibcite{yu2016shoe}{78}
\bibcite{yu2017sketch}{79}
\bibcite{yu2015sketch}{80}
\bibcite{yu2017compressing}{81}
\bibcite{zhang2018shufflenet}{82}
\bibcite{zhong2017practical}{83}
\bibcite{zhou2016less}{84}
\bibcite{zhou2016dorefa}{85}
\bibcite{zoph2017learning}{86}
