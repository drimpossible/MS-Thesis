\noindent Convolutional Neural Networks (CNNs) have found applications in many vision-related domains ranging such as generic image-understanding to self-driving cars \cite{bojarski2016end}. However, deep neural networks are increasingly computationally and memory intensive, and need to work with embedded hardware which have limited energy and memory availability. To address this problem, we presented algorithms for improving the efficiency and accuracy of compressed deep networks. In this thesis, we developed three improvements over existing network compression literature. Distribution Aware Binary Networks, which offers a binary representation specific to the input weight distribution. Hybrid Binary Networks, where we demonstrated that binarizing the right areas in the network contributes significantly to the overall accuracy of the network and does not damage its speed-ups in Hybrid Binary Networks. Deep Expander Networks provided a principled method to prune networks before considering the training data. These methods share a common principle: utilizing over-parameterization in deep neural networks for developing fast, accurate and compact deep networks.\\

\noindent We first showed that binary networks have similar representation power as infinite-precision networks. We proposed a distribution-aware binary representation for layer-weights. Using dynamic programming, we came up with an algorithm for computing this representation efficiently. We got significant improvements on large-scale datasets, showing the efficacy of our algorithm. We provide inuitions and reductions to previous binarization techniques.\\

\noindent Secondly, we presented the idea that avoiding binarization of few select layers can restore the accuracy boost, while nearly maintaining the compression rates. We proposed a heuristic for selecting the layers to be binarized. We successfully weight-binarized the last layers without significant accuracy drops, a problem faced by previous works in this area.  We also showed that we can successfully combine  the  advantages  of  our  approach  with  other  architectural compression strategies, to obtain highly efficient models with negligible accuracy penalties. Overall, we asked the question: Where to binarize to augment our previously proposed binarization strategy.\\

\noindent There can be several possible directions to extend this work. We see promise in binary networks and hope to further develop binary representations which are competitive with floating point representations in accuracy.\\ 

\noindent Finally, we attack pruning in a novel fashion. We provide a principled approach for pruning before training by utilizing concepts from graph theory: particularly using expander graphs that give strong theoretical guarantees on connectivity. The resulting architecture (X-Net) is shown to be highly efficient in terms of both computational requirements and model size, achieve significant improvements over the state-of-the-art architectures in performance on a parameter or run-time budget. The principle used is: global information flows can be maintained even while sparsifying the model significantly, producing efficient deep networks.  We hope that this work motivates other approaches to utilize results from graph theory to develop efficient network architectures. Further interesting directions include using random graphs for architecture design, combine pruning and binarization to provide extremely compact networks, etc.