\noindent CNNs have a large number of redundant parameters, increasing memory and computational cost. Since these applications would be deployed on resource-constrained systems, CNN compression is an important emerging area for research on vision applications \cite{courbariaux2016binarized,zhou2016dorefa, han2015deep, liu2017learning, moczulski2015acdc, yang2015deep, he2016deep, iandola2016squeezenet}. In this chapter, we briefly point out the two areas which are similar to our work and mention some of the important contributions made in network compression literature. They are categorized as follows: (i) Network Pruning (ii) Network Quantization. We also describe hardware accelerators  available for quantized networks to tie-in with some proposed algorithms. We detail approaches like efficient network design and using low-rank approximations to reduce the  number of parameters of a network. A detailed overview of approaches to network compression can be found in surveys such as \cite{cheng2018recent, sze2017efficient}.
 
\section{Network Pruning} 

\noindent Some of the earliest work in network pruning were optimal brain damage \cite{lecun1990optimal} and optimal brain surgeon \cite{hassibi1993optimal} used a function of the Hessian matrix of the loss function to prune a network by reducing the number of connections. With the recent sucess of deep learning, network pruning methods have risen to prominence. Weight-level pruning has the highest compression rate while filter-level pruning is easier to practically exploit , with compression rates not far behind the former. Hence, filter-level pruning is currently considered superior \cite{liu2017learning}. 

\subsection{Pixel and Vector Pruning}

\noindent Optimal brain damage and optimal brain surgeon both belonged to this category. They pruned individual parameters of a network. The problem of pruning weights during train-time have been extensively studied in the literature \cite{sze2017efficient}. Deep compression \cite{han2015learning} also used pruning to achieve compression by an order of magnitude in various standard neural networks. SSL \cite{wen2016structured} regularizes filter values and shapes, along with depth structures. It also performs structured pruning in a channel-wise and filter-wise manner. Dynamic network surgery \cite{guo2016dynamic} builds upon these methods to incorporate connection splicing into pruning, implementing the pruning process in a dynamic way and achieving state-of-the-art compact networks.

\subsection{Channel and Filter Pruning}

\noindent Srinivas \etal \cite{srinivas2015data} propose a systematic data-free method to remove neurons instead of removing individual connections as above. Li \etal \cite{li2016pruning} removed entire filters in the network along with their connecting feature maps, reducing the computation costs significantly. ThiNet \cite{luo2017thinet} proposed to prune filters from a CNN to enable more efficient acceleration. He \etal \cite{he2017channel} propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression-based channel selection procedure for weights and minimize feature map errors by least square reconstruction.  Network slimming \cite{liu2017learning} proposed to zero out channels of inputs based on the scaling factor of the batch normalization layer as a measure of importance of each channel in the input.\\

\noindent Groupwise brain damage \cite{lebedev2016fast} prunes the convolutional kernel tensor in a group-wise fashion by adding group-level sparsity regularization to the standard training process. Group sparse regularization \cite{scardapane2017group} proposed a sparse group lasso penalty in order to impose group-level sparsity on the network’s connections. Group-level pruning has a practical speed-up which scales linearly with the sparsity level.

\section{Quantization}

\noindent There has been a major body of work that quantizes the networks at train-time to achieve efficiency, explored in the literature survey by Yunhui Guo \cite{guo2018survey}.

\subsection{Higher bit quantization}

\noindent HashedNets \cite{chen2015compressing} performed binning of network weights using hash functions. Deep compression \cite{han2015deep} introduced quantization into the current literature by employing trained quantization and Huffman coding to reduce the non-runtime memory from Han \etal \cite{han2015learning}. Zhou \etal \cite{zhou2017inq} quantized networks to 4-bit weights, achieving 8x memory compression by using 4 bits to represent 16 different values and 1 bit to represent zeros. Trained ternary quantization \cite{zhu2016trained} uses 2-bit weights and scaling factors to compress the model by 16x with little accuracy degradation.  Ternary weight networks \cite{li2016ternary} optimize a threshold-based ternary function for approximation, with stronger expressive abilities than binary networks.  The above works are limited in the sense that they cannot leverage most speedups obtained by quantization since general purpose hardware supports only 8-bit integers or 1-bit booleans. Binary networks, on the other hand gain speedups by \texttt{xnor-popcount} operations which could be performed on dedicated hardware. Similarly, they can be practically compressed by using the existing boolean datatype.

\subsection{1-bit quantization}

\noindent BinaryConnect \cite{courbariaux2015binaryconnect} was one of the first works to use binary (+1, -1) values for network parameters, achieving significant compression. Binary neural networks \cite{courbariaux2016binarized} quantized weights and activations of neural networks and achieved near full-precision accuracies for datasets like MNIST, SVHN and CIFAR10. Quantized neural networks\cite{hubara2017quantized} use low-precision quantized weights and inputs and replace floating-point arithmetic operations with bit-wise ones. XNOR-Nets \cite{rastegari2016xnor} followed the work of BNNs \cite{hubara2017quantized}, binarizing both layer weights and inputs and multiplying them with scaling constants - bringing significant accuracy improvements. DoReFa-Net \cite{zhou2016dorefa} used low bit-width gradients during backpropagation, and obtained train-time speedups. HWGQ-Net \cite{cai2017deep} introduces a better suited activation function substituting sign function for better training of quantized networks.  Recent research has proposed a variety of additional methods: including novel activation functions \cite{cai2017deep}, fixed-point bit-width allocations \cite{lin2016fixed}, etc. HTCBN \cite{tang2017train} introduce helpful techniques such as replacing ReLU layers with PReLU layers and a scale layer to recover accuracy loss on binarizing the last layer. Hou \etal \cite{hou2016loss} use Hessian approximations to minimize loss w.r.t. binary weights during training. Anderson \etal \cite{anderson2017high} offer a theoretical analysis of the workings of binary networks in terms of high-dimensional geometry.\\

\noindent Merolla \etal \cite{merolla2016deep} show that binary networks during testing exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections. Further works have extended this in various directions, including using local binary patterns \cite{juefei2016local} which proposed an LBC layer, comprising of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process. Lookup-based compression methods \cite{bagherinezhad2016lcnn} extended that by encoding convolutions by few lookups to a dictionary that is trained to cover the space of weights in CNNs - with training jointly learning a dictionary and a small set of linear combinations of the it.

\section{Other network compression approaches}

\noindent We cover several other aspects explored in the network compression literature, such as low-rank approximations, efficient network architecture design for image recognition and other tasks like sketch recognition. 

\subsection{Low-rank Approximations} 

\noindent Several methods since \cite{sainath2013low} have been introduced to compress pretrained networks as well as train-time compression by approximation weight matrices by their low-rak approximations \cite{novikov2015tensorizing, masana2017domain}. \cite{jaderberg2014speeding} approximate a learnt full rank filter bank as combinations of a rank-1 filter basis ($w∗h$ into $w∗1$ and $1∗h$ filters). Denil \etal \cite{denil2013predicting} use a low-rank approximation using SVD to reduce the network redundancy.  \cite{kim2015compression} used Tucker decomposition method to get the low-rank approxmation. \cite{lebedev2014speeding} developed a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors using non-linear least squares to replace the original layer.

\subsection{Sketch Recognition}

\noindent Works before \cite{yu2015sketch} did not lead to good results by applying image classification architectures, since they are better suited to images than sketches \cite{yu2015sketch}. Sketches require specialized, fine-tuned networks since they have significantly different characteristics as compared to images. Sketch-a-Net from Yu \etal. \cite{yu2015sketch, yu2017sketch} took these factors into account, and proposed a carefully designed network structure that suited sketch representations. Their model showed tremendous increments over the then state-of-the-art, and managed to beat the average human performance using late and early fusion of ensembles. This model has been adopted by a number of later works such as Bui \etal. \cite{bui2016sbir}, Yu \etal \cite{yu2016shoe}, Wang \etal \cite{wang2016crossir}.

\subsection{Efficient Architecture Design}

\noindent Currently there is extensive interest in developing novel convolutional layers/blocks and effectively leveraging them to improve architectures like \cite{iandola2016squeezenet, howard2017mobilenets, hu2017squeeze}. In contrast, approaches like \cite{szegedy2015going} try to design the macro-architectures by connecting pre-existing blocks. Recent concurrent work has been on performing architecture searches effectively \cite{liu2017hierarchical, zoph2017learning,zhong2017practical, liu2017progressive}. Our work is complementary to architecture search techniques as we can leverage their optimized macro-architectures.\\

\noindent Another line of efficient architecture design is grouped convolutions, first proposed in AlexNet\cite{alex2012alexnet} and popularized by MobileNets\cite{howard2017mobilenets} and XCeption\cite{chollet2017xception} architectures .  \cite{zhang2018shufflenet, sandler2018inverted, huang2018condensenet} are the latest works in this area.\\

\noindent It is interesting to note that recent breakthroughs in designing accurate deep networks \cite{he2016deep, huang2017densely} were made by introducing additional connectivity to enable the efficient flow of information through deep networks. This enables the training of compact, accurate deep networks. These approaches, along with grouped convolutions are closely related to our approach. 

\section{Novelty}

\noindent We  ask  the question:  Do  CNNs  need  the  representational power of 32-bit floating point operations? Is it possible to cut down memory costs and make output computations significantly less expensive? Previous literature performed binarization independent of the distribution weights. In Chapter 3, we introduce distribution-aware binarization method which depends on the distribution of weights to calculate an optimal binarization.\\

\noindent Unlike previous work in this area, we look at binarizing specific parts of a network, instead of simply binarizing the inputs to all the layers end-to-end. We see in Chapter 4, binarizing the right areas in the network contributes significantly to the overall accuracy of the network without compromising on speed-ups.\\

\noindent To the best of our knowledge, the method we propose in Chapter 5 is the first attempt at constraining neural network connections by graph-theoretic approaches to improve deep network architecture designs. We differ from previous work by not pruning weights during training, and still achieve comparable performance.
