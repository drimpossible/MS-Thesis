\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Quantization}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Compact layers \(Pruning\)}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Contributions}{chapter.1}% 4
\BOOKMARK [0][-]{chapter.2}{Related Work}{}% 5
\BOOKMARK [1][-]{section.2.1}{Network Pruning}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.1.1}{Pixel and Vector Pruning}{section.2.1}% 7
\BOOKMARK [2][-]{subsection.2.1.2}{Channel and Filter Pruning}{section.2.1}% 8
\BOOKMARK [1][-]{section.2.2}{Quantization}{chapter.2}% 9
\BOOKMARK [2][-]{subsection.2.2.1}{Higher bit quantization}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.2}{1-bit quantization}{section.2.2}% 11
\BOOKMARK [1][-]{section.2.3}{Other network compression approaches}{chapter.2}% 12
\BOOKMARK [2][-]{subsection.2.3.1}{Low-rank Approximations}{section.2.3}% 13
\BOOKMARK [2][-]{subsection.2.3.2}{Sketch Recognition}{section.2.3}% 14
\BOOKMARK [2][-]{subsection.2.3.3}{Efficient Architecture Design}{section.2.3}% 15
\BOOKMARK [1][-]{section.2.4}{Novelty}{chapter.2}% 16
\BOOKMARK [0][-]{chapter.3}{Distribution Aware Binarization of Networks}{}% 17
\BOOKMARK [1][-]{section.3.1}{Representational Power of Binary Networks}{chapter.3}% 18
\BOOKMARK [1][-]{section.3.2}{Distribution-Aware Binarization}{chapter.3}% 19
\BOOKMARK [2][-]{subsection.3.2.1}{Derivation}{section.3.2}% 20
\BOOKMARK [2][-]{subsection.3.2.2}{Intuitions about DAB-Net}{section.3.2}% 21
\BOOKMARK [2][-]{subsection.3.2.3}{Implementation}{section.3.2}% 22
\BOOKMARK [3][-]{section*.10}{Parallel Prefix-Sums to Obtain K}{subsection.3.2.3}% 23
\BOOKMARK [3][-]{section*.11}{Forward and Backward Pass}{subsection.3.2.3}% 24
\BOOKMARK [2][-]{subsection.3.2.4}{Training Procedure}{section.3.2}% 25
\BOOKMARK [1][-]{section.3.3}{Experiments}{chapter.3}% 26
\BOOKMARK [2][-]{subsection.3.3.1}{Experimental Setup}{section.3.3}% 27
\BOOKMARK [2][-]{subsection.3.3.2}{Results}{section.3.3}% 28
\BOOKMARK [2][-]{subsection.3.3.3}{XNOR-Net vs DAB-Net}{section.3.3}% 29
\BOOKMARK [3][-]{section*.16}{Variation of \040and \040across Time}{subsection.3.3.3}% 30
\BOOKMARK [3][-]{section*.17}{Variation of K across Time and Layers}{subsection.3.3.3}% 31
\BOOKMARK [1][-]{section.3.4}{Summary}{chapter.3}% 32
\BOOKMARK [0][-]{chapter.4}{Hybrid Binary Networks}{}% 33
\BOOKMARK [1][-]{section.4.1}{Hybrid Binarization}{chapter.4}% 34
\BOOKMARK [2][-]{subsection.4.1.1}{Error Metric: Optimizing Speed \046 Accuracy}{section.4.1}% 35
\BOOKMARK [2][-]{subsection.4.1.2}{Partitioning Algorithm}{section.4.1}% 36
\BOOKMARK [2][-]{subsection.4.1.3}{Impact on Speed and Energy Use}{section.4.1}% 37
\BOOKMARK [1][-]{section.4.2}{Experiments and Results}{chapter.4}% 38
\BOOKMARK [2][-]{subsection.4.2.1}{Datasets and Models}{section.4.2}% 39
\BOOKMARK [2][-]{subsection.4.2.2}{Results}{section.4.2}% 40
\BOOKMARK [2][-]{subsection.4.2.3}{Algorithmic Insights}{section.4.2}% 41
\BOOKMARK [2][-]{subsection.4.2.4}{Why are layer-wise errors independent?}{section.4.2}% 42
\BOOKMARK [2][-]{subsection.4.2.5}{Variation with the Hybridization Ratio \(R\)}{section.4.2}% 43
\BOOKMARK [2][-]{subsection.4.2.6}{Optimizing Memory}{section.4.2}% 44
\BOOKMARK [2][-]{subsection.4.2.7}{Compressing Compact Models}{section.4.2}% 45
\BOOKMARK [1][-]{section.4.3}{Summary}{chapter.4}% 46
\BOOKMARK [0][-]{chapter.5}{Deep Expander Networks}{}% 47
\BOOKMARK [1][-]{section.5.1}{Approach}{chapter.5}% 48
\BOOKMARK [2][-]{subsection.5.1.1}{Graphs and Deep CNNs}{section.5.1}% 49
\BOOKMARK [2][-]{subsection.5.1.2}{Sparse Random Graphs}{section.5.1}% 50
\BOOKMARK [2][-]{subsection.5.1.3}{Measures of Connectivity}{section.5.1}% 51
\BOOKMARK [2][-]{subsection.5.1.4}{Sensitivity of X-Nets}{section.5.1}% 52
\BOOKMARK [1][-]{section.5.2}{Efficient Algorithms}{chapter.5}% 53
\BOOKMARK [2][-]{subsection.5.2.1}{Using Sparse Representation}{section.5.2}% 54
\BOOKMARK [2][-]{subsection.5.2.2}{X-Net based Fast Dense Convolution}{section.5.2}% 55
\BOOKMARK [1][-]{section.5.3}{Experiments and Results}{chapter.5}% 56
\BOOKMARK [2][-]{subsection.5.3.1}{Comparison with Grouped Convolution}{section.5.3}% 57
\BOOKMARK [2][-]{subsection.5.3.2}{Comparison with Efficient CNN Architectures}{section.5.3}% 58
\BOOKMARK [2][-]{subsection.5.3.3}{Comparison with Pruning Techniques}{section.5.3}% 59
\BOOKMARK [2][-]{subsection.5.3.4}{Stability of Models}{section.5.3}% 60
\BOOKMARK [2][-]{subsection.5.3.5}{Training Wider and Deeper networks}{section.5.3}% 61
\BOOKMARK [1][-]{section.5.4}{Summary}{chapter.5}% 62
\BOOKMARK [0][-]{chapter.6}{Conclusions}{}% 63
\BOOKMARK [0][-]{appendix.A}{Appendix A: Binary Networks: Appendix}{}% 64
\BOOKMARK [1][-]{section.A.1}{Optimal representation of W"0365W}{appendix.A}% 65
\BOOKMARK [1][-]{section.A.2}{Gradient derivation}{appendix.A}% 66
\BOOKMARK [1][-]{section.A.3}{Binary Networks as Approximators}{appendix.A}% 67
\BOOKMARK [1][-]{section.A.4}{Expressibility of Binary Networks}{appendix.A}% 68
\BOOKMARK [1][-]{section.A.5}{Experimental details}{appendix.A}% 69
\BOOKMARK [1][-]{section.A.6}{Experimental details}{appendix.A}% 70
\BOOKMARK [2][-]{subsection.A.6.1}{Data processing}{section.A.6}% 71
\BOOKMARK [2][-]{subsection.A.6.2}{Hyper-parameters}{section.A.6}% 72
\BOOKMARK [1][-]{section.A.7}{FLOPs, Exploiting Filter Repetition and Computational Cost Calculation}{appendix.A}% 73
\BOOKMARK [1][-]{section.A.8}{Models used}{appendix.A}% 74
\BOOKMARK [1][-]{section.A.9}{Model Architectures}{appendix.A}% 75
\BOOKMARK [1][-]{section.A.10}{Binarization-errors across layers }{appendix.A}% 76
\BOOKMARK [0][-]{appendix.B}{Appendix B: Deep Expander Networks: Appendix}{}% 77
\BOOKMARK [1][-]{section.B.1}{Explicit Expanders}{appendix.B}% 78
\BOOKMARK [2][-]{subsection.B.1.1}{Cayley Expander}{section.B.1}% 79
\BOOKMARK [1][-]{section.B.2}{Sensitivity in Expanders}{appendix.B}% 80
\BOOKMARK [1][-]{section.B.3}{Model Details}{appendix.B}% 81
\BOOKMARK [2][-]{subsection.B.3.1}{Filter structure of AlexNet and VGG}{section.B.3}% 82
\BOOKMARK [2][-]{subsection.B.3.2}{Results}{section.B.3}% 83
\BOOKMARK [2][-]{subsection.B.3.3}{Experimental Details}{section.B.3}% 84
\BOOKMARK [0][-]{appendix*.52}{Bibliography}{}% 85
