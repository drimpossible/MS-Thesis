\addvspace {1em}
\contentsline {figure}{\numberline {1.1}{\ignorespaces There are two significant roadblocks to efficient deployment of large CNN models: realtime computation, and in-memory compactness. Cloud is not an option due to these real-time operation constraints.\relax }}{1}{figure.caption.6}
\addvspace {1em}
\addvspace {1em}
\contentsline {figure}{\numberline {3.1}{\ignorespaces An example sketch passing through a convolutional layer filter, with the real-valued filter shown alongside corresponding $\alpha $-$\beta $ and XNOR-Net filters. Orange signifies the highest response areas. We can see that DAB-Net has significantly better responses when compared to XNOR-Net\relax }}{12}{figure.caption.9}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Sub-figures (1) to (4) show the train-time variation of $\alpha $ and $\beta $ for a layer filter. Initially, $\alpha $ and $\beta $ have nearly equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that $\alpha $ and $\beta $ have widely different magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.\relax }}{22}{figure.caption.14}
\contentsline {figure}{\numberline {3.3}{\ignorespaces (1) shows the variation of the normalized K-value over time during training. It falls initially but converges eventually to 0.35. The normalized K-value for XNOR-Net remains almost at 0.5 till the end. (2) shows the variation of normalized K values on random filters across layers. The K-value corresponding to DAB-Net varies across layers based on the distribution of weights of the specific layer, which is not captured by XNOR-Net.\relax }}{23}{figure.caption.15}
\addvspace {1em}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Convolution of binary and non-binary activations of two different layers. Note that the error introduced due to binarization is minimal in the first pair compared to the second. Hence, efficiently deciding \textit {which} layers to binarize could contribute significantly to the overall accuracy of the network and not damage the speed-ups.\relax }}{24}{figure.caption.18}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Binarization-error metric across layers for Sketch-A-Net, ResNet-18, and SqueezeNet. Stars indicate that the layer was replaced with a WeightBinConv layer, while squares indicate the FullBinConv layer was retained in the FBin model. We see that the algorithm selects the last layers in the case of Sketch-A-Net and ResNet, while in the case of SqueezeNet, it selects the first four, last three and some alternate intermediate layers to be replaced by WeightBinConv layers, retaining the rest as FullBinConv layers.\relax }}{25}{figure.caption.19}
\contentsline {figure}{\numberline {4.3}{\ignorespaces The Procedure: Error metrics from binarization of inputs to the network layers are partitioned into clusters using K-means. The highest error cluster indicates the inputs that are not binarized to generate the hybrid version.\relax }}{28}{figure.caption.20}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Trade-off between WeightBinConv layers and accuracy on the TU-Berlin dataset is shown in the left figure, while the trade-off between weight binarized layers and speedup is shown in the right figure. Early on, we observe that a small increase in the percentage of WeightBinConv layers leads to a large increase in accuracy and a marginal decrease in speed. We achieve accuracies comparable to the WBin model with much fewer WeightBinConv layers. \relax }}{33}{figure.caption.24}
\addvspace {1em}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Popular sparse approximations are agnostic to the global information flow in a network, possibly creating disconnected components. In contrast, expander graph-based models produce sparse yet highly connected networks.\relax }}{37}{figure.caption.28}
\contentsline {figure}{\numberline {5.2}{\ignorespaces The proposed fast convolution algorithm for X-Conv layer. We represent all the non-zero filters in the weight matrix of the X-Conv layer as a compressed dense matrix of $D$ channels. The algorithm starts by selecting $D$ channels from input (with replacement) using a mask created while initializing the model. The output is computed by convolving these selected channels with the compressed weight matrices.\relax }}{40}{figure.caption.29}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison between Grouped convolutions and X-Conv using MobileNet architecture trained on ImageNet. X-$d$ or G-$d$ represents the 1x1 conv layers are compressed by $d$ times using X-Conv or Groups. We observe X-MobileNets beat Group-MobileNet by 4\% in accuracy on increasing sparsity.\relax }}{42}{figure.caption.30}
\contentsline {figure}{\numberline {5.4}{\ignorespaces We show the error as a function of \#FLOPs during test-time (below) for DenseNet-BC with X-DenseNet-BCs on CIFAR10 and CIFAR100 datasets. We observe X-DenseNet-BCs achieve better performance tradeoffs over DenseNet-BC models. For each datapoint, we mention the X-C-D-G notation (see Section \ref {sec:denres}) along with the accuracy. \relax }}{43}{figure.caption.31}
\contentsline {figure}{\numberline {5.5}{\ignorespaces We show the error as a function of \#FLOPs to compare between ResNet and X-ResNet on the ImageNet dataset. We observe X-ResNets achieve better performance tradeoffs over original ResNet models.\relax }}{44}{figure.caption.32}
\contentsline {figure}{\numberline {5.6}{\ignorespaces We show the performance tradeoff obtained on training significantly wider and deeper networks on CIFAR-100 dataset. Every datapoint is X-$C$ specified along with the number of parameters, $C$ being the compression factor. We show that training wider or deeper networks along with more compression using X-Nets achieve better accuracies with upto two-thirds of the total parameter and FLOPs on CIFAR-100 dataset. \relax }}{48}{figure.caption.37}
\addvspace {1em}
\addvspace {1em}
\contentsline {figure}{\numberline {A.1}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of Sketch-A-Net. Our hybrid versions replace most conv layers with FullBinConv layers, but replace layers towards the end with WeightBinConv layers, following the algorithm.\relax }}{63}{figure.caption.43}
\contentsline {figure}{\numberline {A.2}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of ResNet-18.\relax }}{64}{figure.caption.44}
\contentsline {figure}{\numberline {A.3}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of SqueezeNet.\relax }}{65}{figure.caption.45}
\contentsline {figure}{\numberline {A.4}{\ignorespaces Comparing architectures of FPrec, Wbin, Fbin, and two Hybrid versions of AlexNet.\relax }}{66}{figure.caption.46}
\addvspace {1em}
