\begin{thebibliography}{10}\setlength{\itemsep}{-1ex}\small

\bibitem{anderson2017high}
A.~G. Anderson and C.~P. Berg.
\newblock The high-dimensional geometry of binary neural networks.
\newblock {\em arXiv preprint arXiv:1705.07199}, 2017.

\bibitem{zhou2017inq}
Z.~Aojun, Y.~Anbang, G.~Yiwen, X.~Lin, and C.~Yurong.
\newblock Incremental network quantization: Towards lossless cnns with
  low-precision weights.
\newblock In {\em ICLR}, 2017.

\bibitem{bagherinezhad2016lcnn}
H.~Bagherinezhad, M.~Rastegari, and A.~Farhadi.
\newblock Lcnn: Lookup-based convolutional neural network.
\newblock {\em CVPR}, 2017.

\bibitem{bojarski2016end}
M.~Bojarski, D.~Del~Testa, D.~Dworakowski, B.~Firner, B.~Flepp, P.~Goyal, L.~D.
  Jackel, M.~Monfort, U.~Muller, J.~Zhang, et~al.
\newblock End to end learning for self-driving cars.
\newblock {\em arXiv preprint arXiv:1604.07316}, 2016.

\bibitem{bui2016sbir}
T.~Bui, L.~Ribeiro, M.~Ponti, and J.~P. Collomosse.
\newblock Generalisation and sharing in triplet convnets for sketch based
  visual search.
\newblock {\em CoRR}, abs/1611.05301, 2016.

\bibitem{cai2017deep}
Z.~Cai, X.~He, J.~Sun, and N.~Vasconcelos.
\newblock Deep learning with low precision by half-wave gaussian quantization.
\newblock {\em CVPR}, 2017.

\bibitem{chen2015compressing}
W.~Chen, J.~Wilson, S.~Tyree, K.~Weinberger, and Y.~Chen.
\newblock Compressing neural networks with the hashing trick.
\newblock In {\em ICML}, 2015.

\bibitem{cheng2018recent}
J.~Cheng, P.-s. Wang, G.~Li, Q.-h. Hu, and H.-q. Lu.
\newblock Recent advances in efficient computation of deep convolutional neural
  networks.
\newblock {\em Frontiers of Information Technology \& Electronic Engineering},
  2018.

\bibitem{cheng2015exploration}
Y.~Cheng, F.~X. Yu, R.~S. Feris, S.~Kumar, A.~Choudhary, and S.-F. Chang.
\newblock An exploration of parameter redundancy in deep networks with
  circulant projections.
\newblock In {\em CVPR}, pages 2857--2865, 2015.

\bibitem{zhu2016trained}
H.~M. Chenzhuo~Zhu, Song~Han and W.~J. Dally.
\newblock Trained ternary quantization.
\newblock In {\em ICLR}, 2017.

\bibitem{chollet2017xception}
F.~Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In {\em CVPR}, 2017.

\bibitem{collins2014memory}
M.~D. Collins and P.~Kohli.
\newblock Memory bounded deep convolutional networks.
\newblock {\em arXiv preprint arXiv:1412.1442}, 2014.

\bibitem{courbariaux2015binaryconnect}
M.~Courbariaux, Y.~Bengio, and J.-P. David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em NIPS}, 2015.

\bibitem{courbariaux2016binarized}
M.~Courbariaux, I.~Hubara, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock In {\em ICML}, 2016.

\bibitem{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em (MCSS)}, 1989.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR}, 2009.

\bibitem{denil2013predicting}
M.~Denil, B.~Shakibi, L.~Dinh, N.~de~Freitas, et~al.
\newblock Predicting parameters in deep learning.
\newblock In {\em NIPS}, pages 2148--2156, 2013.

\bibitem{eitz2012hdhso}
M.~Eitz, J.~Hays, and M.~Alexa.
\newblock How do humans sketch objects?
\newblock {\em ACM Trans. Graph. (Proc. SIGGRAPH)}, 2012.

\bibitem{blocksparse}
S.~Gray, A.~Radford, and D.~P. Kingma.
\newblock Gpu kernels for block-sparse weights.
\newblock {\em arXiv preprint arXiv:1711.09224}, 2017.

\bibitem{guo2018survey}
Y.~Guo.
\newblock A survey on methods and theories of quantized neural networks.
\newblock {\em arXiv preprint arXiv:1808.04752}, 2018.

\bibitem{guo2016dynamic}
Y.~Guo, A.~Yao, and Y.~Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In {\em NIPS}, 2016.

\bibitem{han2015deep}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em ICLR}, 2016.

\bibitem{han2015learning}
S.~Han, J.~Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em NIPS}, pages 1135--1143, 2015.

\bibitem{hassibi1993optimal}
B.~Hassibi, D.~G. Stork, G.~Wolff, and T.~Watanabe.
\newblock Optimal brain surgeon: Extensions and performance comparisons.
\newblock In {\em NIPS}, 1993.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{he2017channel}
Y.~He, X.~Zhang, and J.~Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock {\em CVPR}, 2017.

\bibitem{hoory2006expander}
S.~Hoory, N.~Linial, and A.~Wigderson.
\newblock Expander graphs and their applications.
\newblock {\em BULL. AMER. MATH. SOC.}, 2006.

\bibitem{horowitz2014power}
M.~Horowitz.
\newblock Computing's energy problem (and what we can do about it).
\newblock In {\em International Solid-State Circuits Conference Digest of
  Technical Papers (ISSCC)}, 2014.

\bibitem{hou2016loss}
L.~Hou, Q.~Yao, and J.~T. Kwok.
\newblock Loss-aware binarization of deep networks.
\newblock {\em ICLR}, 2017.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{hu2017squeeze}
J.~Hu, L.~Shen, and G.~Sun.
\newblock Squeeze-and-excitation networks.
\newblock {\em arXiv preprint arXiv:1709.01507}, 2017.

\bibitem{huang2018condensenet}
G.~Huang, S.~Liu, L.~van~der Maaten, and K.~Q. Weinberger.
\newblock Condensenet: An efficient densenet using learned group convolutions.
\newblock In {\em CVPR}, 2018.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~van~der Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, 2017.

\bibitem{hubara2017quantized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock {\em The Journal of Machine Learning Research}, 2017.

\bibitem{iandola2016squeezenet}
F.~N. Iandola, S.~Han, M.~W. Moskewicz, K.~Ashraf, W.~J. Dally, and K.~Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock {\em ICLR}, 2017.

\bibitem{jaderberg2014speeding}
M.~Jaderberg, A.~Vedaldi, and A.~Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock In {\em BMVC}, 2014.

\bibitem{johnson2016densecap}
J.~Johnson, A.~Karpathy, and L.~Fei-Fei.
\newblock Densecap: Fully convolutional localization networks for dense
  captioning.
\newblock In {\em CVPR}, 2016.

\bibitem{juefei2016local}
F.~Juefei-Xu, V.~N. Boddeti, and M.~Savvides.
\newblock Local binary convolutional neural networks.
\newblock {\em CVPR}, 2017.

\bibitem{kim2015compression}
Y.-D. Kim, E.~Park, S.~Yoo, T.~Choi, L.~Yang, and D.~Shin.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock {\em arXiv preprint arXiv:1511.06530}, 2015.

\bibitem{cudaconvnet}
A.~Krizhevsky.
\newblock Cuda-convnet: High-performance c++/cuda implementation of
  convolutional neural networks.
\newblock 2012.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{alex2012alexnet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NIPS}. 2012.

\bibitem{lebedev2014speeding}
V.~Lebedev, Y.~Ganin, M.~Rakhuba, I.~Oseledets, and V.~Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock {\em arXiv preprint arXiv:1412.6553}, 2014.

\bibitem{lebedev2016fast}
V.~Lebedev and V.~Lempitsky.
\newblock Fast convnets using group-wise brain damage.
\newblock In {\em CVPR}, 2016.

\bibitem{lecun1990optimal}
Y.~LeCun, J.~S. Denker, and S.~A. Solla.
\newblock Nips.
\newblock chapter Optimal Brain Damage. 1990.

\bibitem{li2016ternary}
F.~Li, B.~Zhang, and B.~Liu.
\newblock Ternary weight networks.
\newblock {\em arXiv preprint arXiv:1605.04711}, 2016.

\bibitem{li2016pruning}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em ICLR}, 2017.

\bibitem{lin2016fixed}
D.~Lin, S.~Talathi, and S.~Annapureddy.
\newblock Fixed point quantization of deep convolutional networks.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning}, 2016.

\bibitem{lin2017does}
H.~W. Lin, M.~Tegmark, and D.~Rolnick.
\newblock Why does deep and cheap learning work so well?
\newblock {\em Journal of Statistical Physics}, 2017.

\bibitem{liu2017progressive}
C.~Liu, B.~Zoph, J.~Shlens, W.~Hua, L.-J. Li, L.~Fei-Fei, A.~Yuille, J.~Huang,
  and K.~Murphy.
\newblock Progressive neural architecture search.
\newblock {\em arXiv preprint arXiv:1712.00559}, 2017.

\bibitem{liu2017hierarchical}
H.~Liu, K.~Simonyan, O.~Vinyals, C.~Fernando, and K.~Kavukcuoglu.
\newblock Hierarchical representations for efficient architecture search.
\newblock {\em arXiv preprint arXiv:1711.00436}, 2017.

\bibitem{liu2017learning}
Z.~Liu, J.~Li, Z.~Shen, G.~Huang, S.~Yan, and C.~Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock {\em ICCV}, 2017.

\bibitem{luo2017thinet}
J.-H. Luo, J.~Wu, and W.~Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em CVPR}, 2017.

\bibitem{masana2017domain}
M.~Masana, J.~van~de Weijer, L.~Herranz, A.~D. Bagdanov, and J.~Malvarez.
\newblock Domain-adaptive deep network compression.
\newblock {\em Network}, 2017.

\bibitem{merolla2016deep}
P.~Merolla, R.~Appuswamy, J.~V. Arthur, S.~K. Esser, and D.~S. Modha.
\newblock Deep neural networks are robust to weight binarization and other
  non-linear distortions.
\newblock {\em CoRR}, 2016.

\bibitem{mishra2012top}
A.~Mishra, K.~Alahari, and C.~Jawahar.
\newblock Top-down and bottom-up cues for scene text recognition.
\newblock In {\em CVPR}, 2012.

\bibitem{moczulski2015acdc}
M.~Moczulski, M.~Denil, J.~Appleyard, and N.~de~Freitas.
\newblock Acdc: A structured efficient linear layer.
\newblock {\em ICLR}, 2016.

\bibitem{neumann2012real}
L.~Neumann and J.~Matas.
\newblock Real-time scene text localization and recognition.
\newblock In {\em CVPR}, 2012.

\bibitem{novikov2015tensorizing}
A.~Novikov, D.~Podoprikhin, A.~Osokin, and D.~P. Vetrov.
\newblock Tensorizing neural networks.
\newblock In {\em NIPS}, 2015.

\bibitem{rastegari2016xnor}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em ECCV}, 2016.

\bibitem{rolnick2017power}
D.~Rolnick and M.~Tegmark.
\newblock The power of deeper networks for expressing natural functions.
\newblock {\em arXiv preprint arXiv:1705.05502}, 2017.

\bibitem{sainath2013low}
T.~N. Sainath, B.~Kingsbury, V.~Sindhwani, E.~Arisoy, and B.~Ramabhadran.
\newblock Low-rank matrix factorization for deep neural network training with
  high-dimensional output targets.
\newblock In {\em ICASSP}. IEEE, 2013.

\bibitem{sandler2018inverted}
M.~Sandler, A.~Howard, M.~Zhu, A.~Zhmoginov, and L.-C. Chen.
\newblock Inverted residuals and linear bottlenecks: Mobile networks for
  classification, detection and segmentation.
\newblock In {\em CVPR}, 2018.

\bibitem{scardapane2017group}
S.~Scardapane, D.~Comminiello, A.~Hussain, and A.~Uncini.
\newblock Group sparse regularization for deep neural networks.
\newblock {\em Neurocomputing}, 241:81--89, 2017.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em ICLR}, 2015.

\bibitem{spielman2007spectral}
D.~A. Spielman.
\newblock Spectral graph theory and its applications.
\newblock In {\em FOCS}, 2007.

\bibitem{srinivas2015data}
S.~Srinivas and R.~V. Babu.
\newblock Data-free parameter pruning for deep neural networks.
\newblock In {\em BMVC}, 2015.

\bibitem{srinivas2017training}
S.~Srinivas, A.~Subramanya, and R.~V. Babu.
\newblock Training sparse neural networks.
\newblock In {\em CVPRW}, 2017.

\bibitem{sze2017efficient}
V.~Sze, Y.-H. Chen, T.-J. Yang, and J.~S. Emer.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock {\em Proceedings of the IEEE}, 2017.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em CVPR}, 2015.

\bibitem{taigman2014deepface}
Y.~Taigman, M.~Yang, M.~Ranzato, and L.~Wolf.
\newblock Deepface: Closing the gap to human-level performance in face
  verification.
\newblock In {\em CVPR}, 2014.

\bibitem{tang2017train}
W.~Tang, G.~Hua, and L.~Wang.
\newblock How to train a compact binary neural network with high accuracy?
\newblock In {\em AAAI}, 2017.

\bibitem{salil2012pseudo}
S.~P. Vadhan.
\newblock Pseudorandomness.
\newblock {\em Foundations and Trends in Theoretical Computer Science}, 2012.

\bibitem{wang2016crossir}
X.~Wang, X.~Duan, and X.~Bai.
\newblock Deep sketch feature for cross-domain image retrieval.
\newblock {\em Neurocomputing}, 2016.

\bibitem{wen2016structured}
W.~Wen, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In {\em NIPS}, pages 2074--2082. 2016.

\bibitem{yang2015deep}
Z.~Yang, M.~Moczulski, M.~Denil, N.~de~Freitas, A.~Smola, L.~Song, and Z.~Wang.
\newblock Deep fried convnets.
\newblock In {\em CVPR}, pages 1476--1483, 2015.

\bibitem{you2016image}
Q.~You, H.~Jin, Z.~Wang, C.~Fang, and J.~Luo.
\newblock Image captioning with semantic attention.
\newblock In {\em CVPR}, 2016.

\bibitem{yu2016shoe}
Q.~Yu, F.~Liu, Y.-Z. Song, T.~Xiang, T.~M. Hospedales, and C.-C. Loy.
\newblock Sketch me that shoe.
\newblock In {\em CVPR}, 2016.

\bibitem{yu2017sketch}
Q.~Yu, Y.~Yang, F.~Liu, Y.-Z. Song, T.~Xiang, and T.~M. Hospedales.
\newblock Sketch-a-net: A deep neural network that beats humans.
\newblock {\em International Journal of Computer Vision}, 2017.

\bibitem{yu2015sketch}
Q.~Yu, Y.~Yang, Y.-Z. Song, T.~Xiang, and T.~Hospedales.
\newblock Sketch-a-net that beats humans.
\newblock {\em BMVC}.

\bibitem{yu2017compressing}
X.~Yu, T.~Liu, X.~Wang, and D.~Tao.
\newblock On compressing deep models by low rank and sparse decomposition.
\newblock In {\em CVPR}, 2017.

\bibitem{zhang2018shufflenet}
X.~Zhang, X.~Zhou, M.~Lin, and J.~Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In {\em CVPR}, 2018.

\bibitem{zhong2017practical}
Z.~Zhong, J.~Yan, and C.-L. Liu.
\newblock Practical network blocks design with q-learning.
\newblock {\em arXiv preprint arXiv:1708.05552}, 2017.

\bibitem{zhou2016less}
H.~Zhou, J.~M. Alvarez, and F.~Porikli.
\newblock Less is more: Towards compact cnns.
\newblock In {\em ECCV}, 2016.

\bibitem{zhou2016dorefa}
S.~Zhou, Y.~Wu, Z.~Ni, X.~Zhou, H.~Wen, and Y.~Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em ICLR}, 2016.

\bibitem{zoph2017learning}
B.~Zoph, V.~Vasudevan, J.~Shlens, and Q.~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock {\em arXiv preprint arXiv:1707.07012}, 2017.

\end{thebibliography}
